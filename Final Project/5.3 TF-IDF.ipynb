{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a416253190/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(758065, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mlb cincinnati reds t shirt size xl</td>\n",
       "      <td>3</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>4786</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>no description yet</td>\n",
       "      <td>554</td>\n",
       "      <td>859</td>\n",
       "      <td>827</td>\n",
       "      <td>950</td>\n",
       "      <td>950</td>\n",
       "      <td>-0.369464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ava-viv blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>4180</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>adorable top with a hint of lace and a key hol...</td>\n",
       "      <td>934</td>\n",
       "      <td>860</td>\n",
       "      <td>104</td>\n",
       "      <td>950</td>\n",
       "      <td>950</td>\n",
       "      <td>-0.369464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24k gold plated rose</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>4786</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>complete with certificate of authenticity</td>\n",
       "      <td>934</td>\n",
       "      <td>480</td>\n",
       "      <td>584</td>\n",
       "      <td>950</td>\n",
       "      <td>950</td>\n",
       "      <td>0.000978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name  item_condition_id  \\\n",
       "0  mlb cincinnati reds t shirt size xl                  3   \n",
       "1                       ava-viv blouse                  1   \n",
       "2                 24k gold plated rose                  1   \n",
       "\n",
       "                 category_name  brand_name  price  shipping  \\\n",
       "0            Men/Tops/T-shirts        4786   10.0         1   \n",
       "1  Women/Tops & Blouses/Blouse        4180   10.0         1   \n",
       "2      Women/Jewelry/Necklaces        4786   44.0         0   \n",
       "\n",
       "                                    item_description  cat1  cat2  cat3  cat4  \\\n",
       "0                                 no description yet   554   859   827   950   \n",
       "1  adorable top with a hint of lace and a key hol...   934   860   104   950   \n",
       "2          complete with certificate of authenticity   934   480   584   950   \n",
       "\n",
       "   cat5    target  \n",
       "0   950 -0.369464  \n",
       "1   950 -0.369464  \n",
       "2   950  0.000978  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"/mnt/disks/~/clean.csv\")\n",
    "cloth = train[(train.cat1==554)|(train.cat1==934)]\n",
    "print(cloth.shape)\n",
    "cloth.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a416253190/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "#SCALE target variable\n",
    "cloth[\"target\"] = np.log(cloth.price+1)\n",
    "target_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "cloth[\"target\"] = target_scaler.fit_transform(cloth.target.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "TF-IDF computes each word's contribution to a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_item_des = TfidfVectorizer(max_df=0.95, min_df=2, max_features=100, stop_words='english')\n",
    "\n",
    "tfidf_item_desc = tfidf_item_des.fit_transform(cloth['item_description'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tfidf_item_des = TfidfVectorizer(max_df=0.95, min_df=2, max_features=100, stop_words='english')\n",
    "\n",
    "tfidf_name = tfidf_item_des.fit_transform(cloth['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "svd_tfidf = svd.fit_transform(tfidf_item_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 100000 samples in 0.347s...\n",
      "[t-SNE] Computed neighbors for 100000 samples in 355.412s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 21000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 22000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 23000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 24000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 25000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 26000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 27000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 28000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 29000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 30000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 31000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 32000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 33000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 34000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 35000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 36000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 37000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 38000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 39000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 40000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 41000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 42000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 43000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 44000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 45000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 46000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 47000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 48000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 49000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 50000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 51000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 52000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 53000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 54000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 55000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 56000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 57000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 58000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 59000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 60000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 61000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 62000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 63000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 64000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 65000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 66000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 67000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 68000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 69000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 70000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 71000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 72000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 73000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 74000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 75000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 76000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 77000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 78000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 79000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 80000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 81000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 82000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 83000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 84000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 85000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 86000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 87000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 88000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 89000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 90000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 91000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 92000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 93000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 94000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 95000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 96000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 97000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 98000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 99000 / 100000\n",
      "[t-SNE] Computed conditional probabilities for sample 100000 / 100000\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 102.579460\n",
      "[t-SNE] Error after 1000 iterations: 2.704660\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "svd_tfidf = svd_tfidf[:100000]\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=123)\n",
    "tsne_tfidf = tsne_model.fit_transform(svd_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvectorizer = CountVectorizer(min_df=4,\n",
    "                              max_features=10000,\n",
    "                              tokenizer=text_to_word_sequence,\n",
    "                              ngram_range=(1,2))\n",
    "cvz = cvectorizer.fit_transform(cloth['item_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 1 | 100 silk | 12mm | 100 authentic | 18k | 14 based | 18 | 14k yellow | 1 x | 1 black\n",
      "Topic 1: 10 selling | 11 5 | 100 polyester | 10 11 | 100 uv | 14 gauge | 16 inches | 1 business | 18k | 14 based\n",
      "Topic 2: 15 off | 1 large | 1 business | 1 rm | 14k gold | 18k | 1 blue | 14 based | 1 x | 16\n",
      "Topic 3: 16 | 1 1 | 18 inches | 10k | 14 based | 100 uv | 10 worn | 0 | 1 business | 1 black\n",
      "Topic 4: 1 small | 18 20 | 18 | 14 16 | 10 | 14 based | 18k | 1 blue | 1 black | 100 brand\n",
      "Topic 5: 1 grey | 1 business | 120 | 1 black | 18k | 100 cotton | 130 | 1 8 | 1 pink | 12 5\n",
      "Topic 6: 12 5 | 13 | 1 of | 130 | 10mm | 1 size | 1 4 | 15 inches | 13 inches | 14 based\n",
      "Topic 7: 15 inches | 14 based | 1 item | 10mm | 17 | 18k | 1 4 | 10 no | 14g | 100 sure\n",
      "Topic 8: 1 size | 10 12 | 14g surgical | 130 | 11 | 12 | 10mm | 14 based | 1 4 | 15 inches\n",
      "Topic 9: 1 for | 15 | 14k | 10 worn | 18k | 14 based | 11 5 | 1 pair | 10 condition | 12 13\n",
      "Topic 10: 18k | 14g | 14 based | 100 sure | 1 pink | 19 | 1 2 | 1 pair | 00 or | 100\n",
      "Topic 11: 12 13 | 14 | 13 5 | 16 18 | 10 10 | 15 5 | 10 in | 16 5 | 14 based | 10mm\n",
      "Topic 12: 100 | 1 25 | 14 5 | 10mm | 12 22 | 14 based | 1 | 1 business | 00 | 10 14\n",
      "Topic 13: 11 12 | 1 3 | 1 free | 13 inches | 14 based | 18k | 1 x | 100 sure | 10 11 | 1 size\n",
      "Topic 14: 1 black | 10 off | 100 rayon | 13 | 16 | 0 | 00 | 14 based | 18k | 1 x\n",
      "Topic 15: 1 x | 1 business | 18k gold | 1 white | 10 14 | 10 condition | 14 based | 100 brand | 18k | 1\n",
      "Topic 16: 1 time | 1 5 | 1 business | 0 | 13 14 | 10 and | 1 8 | 100 acrylic | 14 based | 130\n",
      "Topic 17: 10 no | 00 | 0 12 | 1 inch | 14k yellow | 0 2 | 120 | 10 5 | 14 based | 10 condition\n",
      "Topic 18: 00 off | 10 but | 10 spandex | 100 cotton | 12 14 | 12 22 | 16 | 14 based | 18 | 1\n",
      "Topic 19: 10mm | 1 4 | 12 medium | 18k | 14 based | 15 inches | 15 5 | 1 | 00 or | 10 condition\n"
     ]
    }
   ],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=20,\n",
    "                                      learning_method='online',\n",
    "                                      max_iter=20,\n",
    "                                      random_state=42)\n",
    "\n",
    "X_topics = lda_model.fit_transform(tfidf_item_desc)\n",
    "topic_summaries = []\n",
    "\n",
    "topic_word = lda_model.components_  # get the topic words\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(10+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>target</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mlb cincinnati reds t shirt size xl</td>\n",
       "      <td>3</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>4786</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>no description yet</td>\n",
       "      <td>554</td>\n",
       "      <td>859</td>\n",
       "      <td>827</td>\n",
       "      <td>950</td>\n",
       "      <td>950</td>\n",
       "      <td>-0.369464</td>\n",
       "      <td>-17.880766</td>\n",
       "      <td>-28.473066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ava-viv blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>4180</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>adorable top with a hint of lace and a key hol...</td>\n",
       "      <td>934</td>\n",
       "      <td>860</td>\n",
       "      <td>104</td>\n",
       "      <td>950</td>\n",
       "      <td>950</td>\n",
       "      <td>-0.369464</td>\n",
       "      <td>-5.985995</td>\n",
       "      <td>12.815629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24k gold plated rose</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>4786</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>complete with certificate of authenticity</td>\n",
       "      <td>934</td>\n",
       "      <td>480</td>\n",
       "      <td>584</td>\n",
       "      <td>950</td>\n",
       "      <td>950</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>-36.932770</td>\n",
       "      <td>4.726030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bundled items requested for ruie</td>\n",
       "      <td>3</td>\n",
       "      <td>Women/Other/Other</td>\n",
       "      <td>4786</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>banana republic bottoms, candies skirt with ma...</td>\n",
       "      <td>934</td>\n",
       "      <td>609</td>\n",
       "      <td>609</td>\n",
       "      <td>950</td>\n",
       "      <td>950</td>\n",
       "      <td>0.076625</td>\n",
       "      <td>-41.921009</td>\n",
       "      <td>5.782502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acacia pacific tides santorini top</td>\n",
       "      <td>3</td>\n",
       "      <td>Women/Swimwear/Two-Piece</td>\n",
       "      <td>79</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "      <td>size small but straps slightly shortened to fi...</td>\n",
       "      <td>934</td>\n",
       "      <td>824</td>\n",
       "      <td>894</td>\n",
       "      <td>950</td>\n",
       "      <td>950</td>\n",
       "      <td>0.097672</td>\n",
       "      <td>31.219269</td>\n",
       "      <td>9.235429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name  item_condition_id  \\\n",
       "0  mlb cincinnati reds t shirt size xl                  3   \n",
       "1                       ava-viv blouse                  1   \n",
       "2                 24k gold plated rose                  1   \n",
       "3     bundled items requested for ruie                  3   \n",
       "4   acacia pacific tides santorini top                  3   \n",
       "\n",
       "                 category_name  brand_name  price  shipping  \\\n",
       "0            Men/Tops/T-shirts        4786   10.0         1   \n",
       "1  Women/Tops & Blouses/Blouse        4180   10.0         1   \n",
       "2      Women/Jewelry/Necklaces        4786   44.0         0   \n",
       "3            Women/Other/Other        4786   59.0         0   \n",
       "4     Women/Swimwear/Two-Piece          79   64.0         0   \n",
       "\n",
       "                                    item_description  cat1  cat2  cat3  cat4  \\\n",
       "0                                 no description yet   554   859   827   950   \n",
       "1  adorable top with a hint of lace and a key hol...   934   860   104   950   \n",
       "2          complete with certificate of authenticity   934   480   584   950   \n",
       "3  banana republic bottoms, candies skirt with ma...   934   609   609   950   \n",
       "4  size small but straps slightly shortened to fi...   934   824   894   950   \n",
       "\n",
       "   cat5    target         t1         t2  \n",
       "0   950 -0.369464 -17.880766 -28.473066  \n",
       "1   950 -0.369464  -5.985995  12.815629  \n",
       "2   950  0.000978 -36.932770   4.726030  \n",
       "3   950  0.076625 -41.921009   5.782502  \n",
       "4   950  0.097672  31.219269   9.235429  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloth = cloth[:100000]\n",
    "t = pd.DataFrame(tsne_tfidf,columns=['t1','t2'])\n",
    "cloth = pd.concat([cloth,t],axis=1)\n",
    "cloth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Maximum values\n",
    "# Base on the histograms, we select the next lengths\n",
    "MAX_NAME_SEQ = 10\n",
    "MAX_ITEM_DESC_SEQ = 100\n",
    "MAX_TEXT = 250\n",
    "MAX_CATEGORY = np.max([np.max(cloth.cat1.max()), \\\n",
    "                       np.max(cloth.cat2.max()), \\\n",
    "                       np.max(cloth.cat3.max())])+3\n",
    "MAX_BRAND = np.max([cloth.brand_name.max()])+1\n",
    "MAX_CONDITION = np.max([cloth.item_condition_id.max()])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99000, 15)\n",
      "(1000, 15)\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT DEVELOPTMENT TEST\n",
    "dtrain, dvalid = train_test_split(cloth, random_state=123, train_size=0.99)\n",
    "print(dtrain.shape)\n",
    "print(dvalid.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Since we are not focusing how to build a neural network, we use a neural network build by ‘noobhound’ from Kaggle. Special acknowledgement to ‘noobhound’, a contributor of Kaggle. We use his neural network model for training the embedded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "def get_keras_data(dataset):\n",
    "    X = {\n",
    "        'brand_name': np.array(dataset.brand_name)\n",
    "        ,'cat1': np.array(dataset.cat1)\n",
    "        ,'cat2': np.array(dataset.cat2)\n",
    "        ,'cat3': np.array(dataset.cat3)\n",
    "        ,'item_condition': np.array(dataset.item_condition_id)\n",
    "        ,'num_vars': np.array(dataset[[\"shipping\"]])\n",
    "        ,'t1': np.array(dataset[[\"t1\"]])\n",
    "        ,'t2': np.array(dataset[[\"t2\"]])\n",
    "    }\n",
    "    return X\n",
    "\n",
    "X_train = get_keras_data(dtrain)\n",
    "X_valid = get_keras_data(dvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "brand_name (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat1 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat2 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat3 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_condition (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 10)        48090       brand_name[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 10)        9440        cat1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 10)        9440        cat2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 10)        9440        cat3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 5)         30          item_condition[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 10)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 10)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 10)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 10)           0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 5)            0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "num_vars (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 46)           0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 num_vars[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          6016        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            65          dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 90,777\n",
      "Trainable params: 90,777\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Neural Network \n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]\n",
    "\n",
    "def rmsle_cust(y_true, y_pred):\n",
    "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    # hyper parameters\n",
    "    dr_r = 0.1\n",
    "    \n",
    "    # Inputs\n",
    "    brand_name = Input(shape=[1], name=\"brand_name\")\n",
    "    cat1 = Input(shape=[1], name=\"cat1\")\n",
    "    cat2 = Input(shape=[1], name=\"cat2\")\n",
    "    cat3 = Input(shape=[1], name=\"cat3\")\n",
    "    item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    t1 = Input(shape=[1], name=\"t1\")\n",
    "    t2 = Input(shape=[1], name=\"t2\")\n",
    "    \n",
    "    # Embeddings layers\n",
    "    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n",
    "    emb_cat1 = Embedding(MAX_CATEGORY, 10)(cat1)\n",
    "    emb_cat2 = Embedding(MAX_CATEGORY, 10)(cat2)\n",
    "    emb_cat3 = Embedding(MAX_CATEGORY, 10)(cat3)\n",
    "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n",
    "    emb_t1 = Embedding(MAX_CATEGORY, 10)(t1)\n",
    "    emb_t2 = Embedding(MAX_CATEGORY, 10)(t2)\n",
    "    \n",
    "    # main layer\n",
    "    main_l = concatenate([\n",
    "        Flatten() (emb_brand_name)\n",
    "        , Flatten() (emb_cat1)\n",
    "        , Flatten() (emb_cat2)\n",
    "        , Flatten() (emb_cat3)\n",
    "        , Flatten() (emb_item_condition)\n",
    "        , num_vars\n",
    "    ])\n",
    "    main_l = Dropout(dr_r) (Dense(128) (main_l))\n",
    "    main_l = Dropout(dr_r) (Dense(64) (main_l))\n",
    "    \n",
    "    # output\n",
    "    output = Dense(1, activation=\"linear\") (main_l)\n",
    "    \n",
    "    # model\n",
    "    model = Model([brand_name, cat1, cat2, cat3, item_condition, \\\n",
    "                   num_vars, t1, t2], output)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "99000/99000 [==============================] - 1s 6us/step - loss: 0.0541 - mean_absolute_error: 0.1887 - rmsle_cust: 0.0223 - val_loss: 0.0373 - val_mean_absolute_error: 0.1523 - val_rmsle_cust: 0.0205\n",
      "Epoch 2/5\n",
      "99000/99000 [==============================] - 0s 3us/step - loss: 0.0391 - mean_absolute_error: 0.1554 - rmsle_cust: 0.0197 - val_loss: 0.0347 - val_mean_absolute_error: 0.1444 - val_rmsle_cust: 0.0205\n",
      "Epoch 3/5\n",
      "99000/99000 [==============================] - 0s 3us/step - loss: 0.0363 - mean_absolute_error: 0.1457 - rmsle_cust: 0.0195 - val_loss: 0.0324 - val_mean_absolute_error: 0.1355 - val_rmsle_cust: 0.0205\n",
      "Epoch 4/5\n",
      "99000/99000 [==============================] - 0s 3us/step - loss: 0.0317 - mean_absolute_error: 0.1361 - rmsle_cust: 0.0195 - val_loss: 0.0276 - val_mean_absolute_error: 0.1285 - val_rmsle_cust: 0.0204\n",
      "Epoch 5/5\n",
      "99000/99000 [==============================] - 0s 3us/step - loss: 0.0291 - mean_absolute_error: 0.1328 - rmsle_cust: 0.0194 - val_loss: 0.0268 - val_mean_absolute_error: 0.1252 - val_rmsle_cust: 0.0201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe0e035eb70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 20000\n",
    "epochs = 5\n",
    "\n",
    "model = get_model()\n",
    "model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n",
    "          , validation_data=(X_valid, dvalid.target)\n",
    "          , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE of testing:  0.6237763615129761\n"
     ]
    }
   ],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
    "    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n",
    "# Source: https://www.kaggle.com/marknagelberg/rmsle-function\n",
    "\n",
    "val_preds = model.predict(X_valid)\n",
    "val_preds = target_scaler.inverse_transform(val_preds)\n",
    "val_preds = np.exp(val_preds)+1\n",
    "\n",
    "y_true = np.array(dvalid.price.values)\n",
    "y_pred = val_preds[:,0]\n",
    "v_rmsle = rmsle(y_true, y_pred)\n",
    "\n",
    "# print(\"RMSLE of training: \", score_train)\n",
    "print(\"RMSLE of testing: \", v_rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save('/mnt/disks/~/model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
